The State of ESG—Using Natural Language Processing to Illuminate Corporate Sustainability Issues.

“Once considered tangential to business decision-making, environmental, social and governance factors have now become business imperatives—and for those companies with foresight, sustainable business strategies are also a pathway to stable, profitable, long-term economic growth and social prosperity.”—ceres.org
“Business as usual is not a viable option.” —Rebecca M Henderson, Reimagining Capitalism in a World on Fire
Corporate sustainability practices are increasingly an important focus for stabilizing and growing a bottom line, amid an ever-changing, significantly interconnected global economy. According to an article in the Harvard Business Review, "Embedded sustainability efforts clearly result in a positive impact on business performance." The article explains how sustainability efforts:
Improve risk management
Foster innovation 
Improve financial performance 
Increase customer loyalty 
Attract and engage employees 
Drive a competitive advantage through positive stakeholder engagement.
But what are those practices? For companies wanting to explore making improvements to their own practices, where should they focus their attention? What practices should be audited and improved? And how can a company already committed to ethical ESG practices stay up stop date on topics of concern? Which topics are the most pressing, in the eye of investors and consumers? I used unsupervised Natural Language Processing (NLP) and ceres.org to find out.
Ceres.org is an organization that tracks the sustainability reporting of companies and compiles shareholder resolutions filed by investment network members. The site hosts an "Engagement Tracker" database that contains thousands of resolution entries, dating from 2009 to present. These resolutions are part of broader investor efforts to encourage companies to address the full scope of environmental, social and governance (ESG) issues.
I obtained a corpus of 923 shareholder resolutions, initially scraping the documents from ceres.org using selenium, then gathering hundreds manually after my selenium program produced an incomplete corpus (will return to troubleshoot automating this download!). Each document included filing status, title, publicly traded organization, the filing party, filing year, and the resolution text. I collected shareholder resolutions from January, 2018 to June, 2022, which contained about 410,000 words in total.
I discovered that similar research was conducted and published in the journal Sustainability by Raghupathi, V. et. al (2020), and my results are examined with a comparative eye to results they achieved. While our research largely follows similar NLP paths, the fundamental questions behind our research vary. Raghupathi, V. et. al (2020) asked, “Do shareholder resolutions reflect corporate sustainability concerns in terms of environmental, social, and governance aspects?”
My research aimed at understanding:
What topics (in what frequency) occur in shareholder sustainability resolutions?
Do proposed resolutions have positive, negative or neutral sentiment scores?
If there is a variety of sentiments, what topics are associated with each sentiment type?
Sentiment score can indicate how the filing party is feeling about the topic. I use sentiment and topic frequency as two proxies for understanding which issues are the most urgent to address, focusing first on topics with the most negative score and topics occurring in the highest frequency.

After obtaining the data, I used regex and lambda functions to removed a standard list of english stop words and special characters. After a few runs through my program, I discovered a handful of words that occurred in high enough frequency that they did not contribute to results in any significant manner, but were still infrequent enough to not be culled by max_df 0.99 removal. I removed those words as well. *make a figure for this Such words included, 'www', 'whereas', 'include', 'full', ‘request', ‘resolved', ‘shareholder', ‘shareholders', 'company', ‘companies', ‘use', 'https', 'http', 'pdf', and ‘one’. I stemmed the words in the documents, relying on the least aggressive stemmer (Porter), but loosely comparing the results of the other stemmers, Lancaster and SnowBall for significant differences in results. 
Once I finished processing the documents, I created a couple of visualizations to preview the stemmed terms by frequency—a wordcloud (using the WordCloud python package) and a line plot (using Matplotlib).

Next, I created a document term matrix (DTM) for uni-gram, bi-gram, and tri-gram analysis, using 

CountVectorizer. I applied Scikit-Learn’s non-negative matrix factorization (NMF) to the DTM, which clustered the terms into use groups. I experimented with the number of groups needed to accurately capture the topics, without repetition (in the case of too many groups) or without ambiguity (in the case of too few groups). I arrived at seven use groups, which I then labeled manually. Here are the results:
Human rights impact assessment is a bit ambiguous, so I did a second round of topic modeling on the documents with that label. I explain this further down in this article.
I created a document-topic matrix then, using nmf again, and assigned the highest scoring topic to each of the documents in the corpus using pandas idxmax. I counted the frequency of topics, the number of resolutions per year, and the count of topics over time (from 2018-2022)*.
Sentiment analysis came next. I explored the sentiment score of each topic using SentimentIntensityAnalyzer from the Vader Sentiment package. It produced a positive score, negative score and compound score for each pre-processed but un-stemmed document. The compound score measures the overall sentiment with a score of 1 being the most positive score a document can have. Here is a plot of all document scores and a plot of average compound score by topic.*
The sentiment score of these topics may reflect those which the shareholders view are the most (and least) grave or egregious topics. Human rights impact assessment is the only topic that received a negative average compound sentiment score, and as mentioned above, the topic could be better understood. To this, I ran a second round of topic modeling on documents that were classified as pertaining to Human Rights Impact Assessment". The results showed that the concerns raged from animal welfare ("Human and Animal Rights" or "Moral/Ethical practices" may have been better topic labels for this category).  Here are the results of the subset topic modeling:*

Applying these results
Conducting a self-audit of a company’s practices is the next step. While this is outside the initial scope of the project, I chose to explore the applicability of some of this knowledge by constructing an app that can be used to explore wage disparity within a company. I downloaded a sample salary dataset from kaggle.com and created a simple streamlit app that used linear regression to predict the salary of a hypothetical employee. All factors were kept the same (age, location, years of work, etc) and two salaries were predicted, one for a male employee and one for a female employee. In this sample dataset, the predicted female wage was consistently measurably less than the male's wage. The app can be accessed here.*
Summary
I found that the top occurring topics in the corpus of resolutions includes: '1. Climate emissions targets', '2. Activism/Lobbying', '3. Female Wage Disparity', '4. Campaign Contributions', '5. ESG management and corporate policies', '6. Plastic Pollution and the Ocean', '7. human rights impact assessments', Results of this research will benefit consultants, companies, investors or activists who wish to understand the current state of sustainability practices and wish to learn from and apply the best practices to their own operations.

Moving forward, I would like to solve the issue I’m having with scraping the documents (dynamically loaded page is proving tricky for me at this time!). It would be great to automate the acquisition of documents to have a larger corpus, allow for more robust exploration over time, and a closer look at the most relevant topics today. I would also like to explore the use of NMF coherence score as a way to stabilize the model with objective optimization of the best number of topics for the corpus.

Discussion
The number of climate focused resolutions is an indication of the trending nature of that concern. The negative sentiment score of “Moral/Ethical practices” may indicate the gravity of the topic and thus the severity of a popular backlash by consumers if a company is found to be violating those ethics. Both can be seen as crucial starting points when evaluating business practices, auditing and altering operational practices. 
Algorithms/Tools
Algorithms used include, Regex removal of special characters and punctuation, removal of stopwords via lambda function and countvectorizor, stemming using Porter, Lancaster, and Snowball for comparison (porter went with), clustering and dimensionality reduction using non-negative matrix factorization and vader sentiment analysis. I used matplotlib and wordcloud to create visualizations.
References:
Raghupathi, V.; Ren, J.; Raghupathi, W. Identifying Corporate Sustainability Issues by Analyzing Shareholder Resolutions: A Machine-Learning Text Analytics Approach. Sustainability 2020, 12, 4753. https://doi.org/10.3390/su12114753






Deleted
ESG consultants can be a high cost line item on the already potentially expensive upfront cost of changing business practices. To create a low cost method for discovering where companies can focus their attention and efforts, I use Natural Language Processing techniques to perform topic modeling of the terms within shareholder resolutions from ceres.org.
